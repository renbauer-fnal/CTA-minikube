@mastersthesis{cristina_msc_thesis,
  author       = {Cristina-Gabriela Moraru},
  title        = {Enhancing the low-level tape layer of {CERN Tape Archive} software},
  school       = {University Politehnica of Bucharest, Hungary},
  year         = {2017},
  month        = {Sept.},
  note         = {\url{https://cds.cern.ch/record/2282014/files/CERN-THESIS-2017-131.pdf}}
}

@misc{rucio,
  title        = {Rucio},
  howpublished = {\url{https://rucio.cern.ch/}}
}

@misc{mhvtl,
  author       = {Mark Harvey},
  title        = {{mhVTL}},
  howpublished = {\url{http://www.mhvtl.com/}}
}

@misc{GFAL2,
  title        = {{Grid File Access Library (GFAL2)}},
  howpublished = {\url{https://dmc.web.cern.ch/projects/gfal-2/home}}
}

@misc{clipper_tco_2015,
  author       = {David Reine and Mike Kahn},
  title        = {{Continuing the search for the right mix of long-term storage infrastructure---a
                   TCO analysis of disk and tape solutions}},
  howpublished = {\url{http://www.clipper.com/research/TCG2015006.pdf}},
  year         = {2015},
  month        = {July}
}

@misc{atlas_future_chep2018,
  author       = {Fernando Barreiro and Doug Benjamin and Taylor Childers and Kaushik De and Johannes Elmsheuser and
                  Andrej Filipcic and Alexei Klimentov and Mario Lassnig and Tadashi Maeno and Danila Oleynik and
                  Sergey Panitkin and Torre Wenaus},
  title        = {{The Future of Distributed Computing Systems in ATLAS: Boldly Venturing Beyond Grids}},
  howpublished = {\url{https://indico.cern.ch/event/587955/contributions/2937395/}},
  year         = {2018},
  month        = {July}
}

@misc{atlas_status_lishep2018,
  author       = {Isabelle Wingerter-Seez},
  title        = {{Highlights from the ATLAS Experiment (LISHEP 2018)}},
  howpublished = {\url{https://indico.cern.ch/event/675301/contributions/3104623/}},
  year         = {2018},
  month        = {Sept.}
}

@misc{xin_zhao_tape_usage,
  author       = {Xin Zhao},
  title        = {\textit{Tape Usage (ADC Technical Coordination Board Meeting)}},
  howpublished = {\url{https://indico.cern.ch/event/732181/contributions/3019046/}},
  year         = {2018},
  month        = {May},
}

@article{cta_chep2016,
  author   = {S. Murray and V. Bahyl and G. Cancio and E. Cano and V. Kotlyar and D. F. Kruse and J. Leduc},
  title    = {An efficient, modular and simple tape archiving solution for {LHC Run--3}},
  journal  = {Journal of Physics: Conference Series},
  volume   = {898},
  number   = {6},
  pages    = {062013},
  url      = {https://stacks.iop.org/1742-6596/898/i=6/a=062013},
  year     = {2017},
  abstract = {The IT Storage group at CERN develops the software responsible for archiving to tape the custodial copy of the physics data generated by the LHC experiments. Physics run 3 will start in 2021 and will introduce two major challenges for which the tape archive software must be evolved. Firstly the software will need to make more efficient use of tape drives in order to sustain the predicted data rate of 150 petabytes per year as opposed to the current 50 petabytes per year. Secondly the software will need to be seamlessly integrated with EOS, which has become the de facto disk storage system provided by the IT Storage group for physics data. The tape storage software for LHC physics run 3 is code named CTA (the CERN Tape Archive). This paper describes how CTA will introduce a pre-emptive drive scheduler to use tape drives more efficiently, will encapsulate all tape software into a single module that will sit behind one or more EOS systems, and will be simpler by dropping support for obsolete backwards compatibility.}
}

@article{CERN_data_chep2016,
  author={X Espinal and E Bocchi and B Chan and A Fiorot and J Iven and G Lo Presti and J Lopez and H Gonzalez and M Lamanna and L Mascetti and J
Moscicki and A Pace and A Peters and S Ponce and H Rousseau and D van der Ster},
  title={CERN data services for LHC computing},
  journal={Journal of Physics: Conference Series},
  volume={898},
  number={6},
  pages={062028},
  url={http://stacks.iop.org/1742-6596/898/i=6/a=062028},
  doi={doi:10.1088/1742-6596/898/6/062028},
  year={2017},
  abstract={Dependability, resilience, adaptability and efficiency. Growing requirements require tailoring storage services and novel solutions. Unprecedented volumes of data coming from the broad number of experiments at CERN need to be quickly available in a highly scalable way for large-scale processing and data distribution while in parallel they are routed to tape for long-term archival. These activities are critical for the success of HEP experiments. Nowadays we operate at high incoming throughput (14GB/s during 2015 LHC Pb-Pb run and 11PB in July 2016) and with concurrent complex production work-loads. In parallel our systems provide the platform for the continuous user and experiment driven work-loads for large-scale data analysis, including end-user access and sharing. The storage services at CERN cover the needs of our community: EOS and CASTOR as a large-scale storage; CERNBox for end-user access and sharing; Ceph as data back-end for the CERN OpenStack infrastructure, NFS services and S3 functionality; AFS for legacy distributed-file-system services. In this paper we will summarise the experience in supporting LHC experiments and the transition of our infrastructure from static monolithic systems to flexible components providing a more coherent environment with pluggable protocols, tuneable QoS, sharing capabilities and fine grained ACLs management while continuing to guarantee dependable and robust services.}
}

@article{eos_chep2015,
  author   = {A. J. Peters and E. A. Sindrilaru and G. Adde},
  title    = {{EOS} as the present and future solution for data storage at {CERN}},
  journal  = {Journal of Physics: Conference Series},
  volume   = {664},
  number   = {4},
  pages    = {042042},
  url      = {http://stacks.iop.org/1742-6596/664/i=4/a=042042},
  year     = {2015},
  abstract = {EOS is an open source distributed disk storage system in production since 2011 at CERN. Development focus has been on low-latency analysis use cases for LHC 1 and non- LHC experiments and life-cycle management using JBOD 2 hardware for multi PB storage installations. The EOS design implies a split of hot and cold storage and introduced a change of the traditional HSM 3 functionality based workflows at CERN. The 2015 deployment brings storage at CERN to a new scale and foresees to breach 100 PB of disk storage in a distributed environment using tens of thousands of (heterogeneous) hard drives. EOS has brought to CERN major improvements compared to past storage solutions by allowing quick changes in the quality of service of the storage pools. This allows the data centre to quickly meet the changing performance and reliability requirements of the LHC experiments with minimal data movements and dynamic reconfiguration. For example, the software stack has met the specific needs of the dual computing centre set-up required by CERN and allowed the fast design of new workflows accommodating the separation of long-term tape archive and disk storage required for the LHC Run II. This paper will give a high-level state of the art overview of EOS with respect to Run II, introduce new tools and use cases and set the roadmap for the next storage solutions to come.}
}

@article{FTS3,
  author  = {A. A. Ayllon, M. Salichos, M. K. Simon and O. Keeble},
  title   = {FTS3: New Data Movement Service For WLCG},
  journal = {Journal of Physics: Conference Series},
  volume  = {513},
  number  = {3},
  pages   = {032081},
  url     = {http://stacks.iop.org/1742-6596/513/i=3/a=032081},
  year    = {2014}
}

@article{SRM2_2,
  author={F Donno and L Abadie and P Badino and J-P Baud and E Corso and S D Witt and P Fuhrmann and J Gu and B Koblitz and S Lemaitre and M^M
Litmaath and D Litvintsev and G L Presti and L Magnoni and G McCance and T Mkrtchan and R Mollon and V Natarajan and T Perelmutov and D^M
Petravick and A Shoshani and A Sim and D Smith and P Tedesco and R Zappi},
  title={Storage resource manager version 2.2: design, implementation, and testing experience},
  journal={Journal of Physics: Conference Series},
  volume={119},
  number={6},
  pages={062028},
  url={http://stacks.iop.org/1742-6596/119/i=6/a=062028},
  year={2008},
  abstract={Storage Services are crucial components of the Worldwide LHC Computing Grid Infrastructure spanning more than 200 sites and serving computing and storage resources to the High Energy Physics LHC communities. Up to tens of Petabytes of data are collected every year by the four LHC experiments at CERN. To process these large data volumes it is important to establish a protocol and a very efficient interface to the various storage solutions adopted by the WLCG sites. In this work we report on the experience acquired during the definition of the Storage Resource Manager v2.2 protocol. In particular, we focus on the study performed to enhance the interface and make it suitable for use by the WLCG communities. At the moment 5 different storage solutions implement the SRM v2.2 interface: BeStMan (LBNL), CASTOR (CERN and RAL), dCache (DESY and FNAL), DPM (CERN), and StoRM (INFN and ICTP). After a detailed inside review of the protocol, various test suites have been written identifying the most effective set of tests: the S2 test suite from CERN and the SRM-Tester test suite from LBNL. Such test suites have helped verifying the consistency and coherence of the proposed protocol and validating existing implementations. We conclude our work describing the results achieved.}
}

@inproceedings{castor2007,
  author    = {Lo Presti, Giuseppe and Olof Barring and Alasdair Earl and Rosa Maria Garcia Rioja and Sebastien Ponce and Giulia Taurelli and Dennis Waldron and Dos Santos, Miguel Coelho},
  title     = {{CASTOR:} A Distributed Storage Resource Facility for High Performance Data Processing at {CERN}},
  booktitle = {24th {IEEE} Conference on Mass Storage Systems and Technologies {(MSST} 2007), 24--27 September 2007, San Diego, California, {USA}},
  pages     = {275--280},
  year      = {2007},
  crossref  = {DBLP:conf/mss/2007},
  url       = {http://doi.ieeecomputersociety.org/10.1109/MSST.2007.7},
  doi       = {10.1109/MSST.2007.7},
  timestamp = {Mon, 18 May 2015 17:23:13 +0200},
  biburl    = {https://dblp.org/rec/bib/conf/mss/PrestiBERPTWS07},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@proceedings{DBLP:conf/mss/2007,
  title     = {24th {IEEE} Conference on Mass Storage Systems and Technologies {(MSST} 2007), 24--27 September 2007, San Diego, California, {USA}},
  publisher = {{IEEE} Computer Society},
  year      = {2007},
  url       = {http://ieeexplore.ieee.org/xpl/mostRecentIssue.jsp?punumber=4367953},
  isbn      = {0--7695--3025--7},
  timestamp = {Mon, 18 May 2015 17:23:13 +0200},
  biburl    = {https://dblp.org/rec/bib/conf/mss/2007},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
  	
@article{tapeserver_chep2015,
  author={E Cano and S Murray and D F Kruse and V Kotlyar and D Côme},
  title={The new CERN tape software - getting ready for total performance},
  journal={Journal of Physics: Conference Series},
  volume={664},
  number={4},
  pages={042007},
  url={http://stacks.iop.org/1742-6596/664/i=4/a=042007},
  year={2015},
  abstract={CASTOR (the CERN Advanced STORage system) is used to store the custodial copy of all of the physics data collected from the CERN experiments, both past and present. CASTOR is a hierarchical storage management system that has a disk-based front-end and a tape-based back-end. The software responsible for controlling the tape back-end has been redesigned and redeveloped over the last year and was put in production at the beginning of 2015. This paper summarises the motives behind the redesign, describes in detail the redevelopment work and concludes with the short and long-term benefits.}
}
